
@misc{oshea_introduction_2015,
	title = {An {Introduction} to {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.08458},
	abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {O'Shea, Keiron and Nash, Ryan},
	month = dec,
	year = {2015},
	doi = {10.48550/arXiv.1511.08458},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv:1511.08458},
	file = {Preprint PDF:/home/emma/Zotero/storage/B2QW96RS/O'Shea and Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf:application/pdf;Snapshot:/home/emma/Zotero/storage/W58V56GI/1511.html:text/html},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	doi = {10.48550/arXiv.1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:1505.04597},
	file = {Preprint PDF:/home/emma/Zotero/storage/VXQ7G99Y/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf;Snapshot:/home/emma/Zotero/storage/EP77Y93J/1505.html:text/html},
}

@article{gallant_perceptron-based_1990,
	title = {Perceptron-based learning algorithms},
	volume = {1},
	doi = {10.1109/72.80230},
	abstract = {A key task for connectionist research is the development and analysis of learning algorithms. An examination is made of several supervised learning algorithms for single-cell and network models. The heart of these algorithms is the pocket algorithm, a modification of perceptron learning that makes perceptron learning well-behaved with nonseparable training data, even if the data are noisy and contradictory. Features of these algorithms include speed algorithms fast enough to handle large sets of training data; network scaling properties, i.e. network methods scale up almost as well as single-cell models when the number of inputs is increased; analytic tractability, i.e. upper bounds on classification error are derivable; online learning, i.e. some variants can learn continually, without referring to previous data; and winner-take-all groups or choice groups, i.e. algorithms can be adapted to select one out of a number of possible classifications. These learning algorithms are suitable for applications in machine learning, pattern recognition, and connectionist expert systems.},
	journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
	author = {Gallant, Steve},
	month = feb,
	year = {1990},
	pages = {179--91},
	file = {Full Text PDF:/home/emma/Zotero/storage/H9SPN9H3/Gallant - 1990 - Perceptron-based learning algorithms.pdf:application/pdf},
}

@article{wu_state---art_2024,
	title = {A state-of-the-art survey of {U}-{Net} in microscopic image analysis: from simple usage to structure mortification},
	volume = {36},
	issn = {1433-3058},
	shorttitle = {A state-of-the-art survey of {U}-{Net} in microscopic image analysis},
	url = {https://doi.org/10.1007/s00521-023-09284-4},
	doi = {10.1007/s00521-023-09284-4},
	abstract = {Microscopic image analysis technology helps solve the inadvertences of artificial traditional methods in disease, wastewater treatment, and environmental change monitoring analysis. Convolutional neural network (CNN) play an important role in microscopic image analysis. Image segmentation, in which U-Net is increasingly applied in microscopic image segmentation, is a crucial step in detection, tracking, monitoring, feature extraction, modelling, and analysis. This paper comprehensively reviews the development history of U-Net, analyses several research results of various segmentation methods since the emergence of U-Net, and conducts a comprehensive review of related papers. This paper summarised the improved methods of U-Net and then listed the existing significance of image segmentation techniques and their improvements introduced over the years. Finally, focusing on the different improvement strategies of U-Net in different papers, the related work of each application target is reviewed according to detailed technical categories to facilitate future research. Researchers can see the dynamics of the transmission of technological development and keep up with future trends in this interdisciplinary field.},
	language = {en},
	number = {7},
	urldate = {2024-10-29},
	journal = {Neural Computing and Applications},
	author = {Wu, Jian and Liu, Wanli and Li, Chen and Jiang, Tao and Shariful, Islam Mohammad and Yao, Yudong and Sun, Hongzan and Li, Xiaoqi and Li, Xintong and Huang, Xinyu and Grzegorzek, Marcin},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Convolutional neural network, Deep learning, Image segmentation, Microscopic image analysis, U-Net},
	pages = {3317--3346},
	file = {Full Text PDF:/home/emma/Zotero/storage/PQHAD3FB/Wu et al. - 2024 - A state-of-the-art survey of U-Net in microscopic image analysis from simple usage to structure mor.pdf:application/pdf},
}

@article{chen_deep_2020,
	title = {Deep learning-based method for {SEM} image segmentation in mineral characterization, an example from {Duvernay} {Shale} samples in {Western} {Canada} {Sedimentary} {Basin}},
	volume = {138},
	doi = {10.1016/j.cageo.2020.104450},
	journal = {Computers \& Geosciences},
	author = {Chen, Zhuoheng and Liu, Xiaojun and Yang, Jijin and Little, Edward and Zhou, Yu},
	month = feb,
	year = {2020},
	pages = {104450},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
    pages = {164--172},
	annote = {http://www.deeplearningbook.org},
}
@incollection{Bishop2006NeuralNetworks,
    author    = {Christopher M. Bishop},
    title     = {Neural Networks},
    booktitle = {Pattern Recognition and Machine Learning},
    series    = {Information Science and Statistics},
    publisher = {Springer New York, NY},
    year      = {2006},
    edition   = {1},
    isbn      = {978-0-387-31073-2},
    pages     = {225--290},
    copyright = {Springer-Verlag New York 2006},
    note      = {eBook Packages: Computer Science, Computer Science (R0)},
}
@article{Parmezan_Perceptron_image,
author = {Parmezan, Antonio and Alves de Souza, Vin√≠cius and Batista, Gustavo},
year = {2019},
month = {01},
pages = {},
title = {Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model},
journal = {Information Sciences},
doi = {10.1016/j.ins.2019.01.076}
}

@article{Convolution_diagram,
author = {Song, Shuhan},
year = {2023},
month = {10},
pages = {},
title = {The use of color elements in graphic design based on convolutional neural network model},
volume = {9},
journal = {Applied Mathematics and Nonlinear Sciences},
doi = {10.2478/amns.2023.2.00536}
}

@article{CNN_clasif,
author = {Abdelsamad, Safa and Abdelteef, Mohammed and Elsheikh, Othman and Ali, Yomna and Elsonni, Tarik and Abdelhaq, Maha and Alsaqour, Raed and Saeed, Rashid},
year = {2023},
month = {05},
pages = {2235},
title = {Vision-Based Support for the Detection and Recognition of Drones with Small Radar Cross Sections},
volume = {12},
journal = {Electronics},
doi = {10.3390/electronics12102235}
}

@article{AMARI1993185,
title = {Backpropagation and stochastic gradient descent method},
journal = {Neurocomputing},
volume = {5},
number = {4},
pages = {185-196},
year = {1993},
issn = {0925-2312},
doi = {https://doi.org/10.1016/0925-2312(93)90006-O},
url = {https://www.sciencedirect.com/science/article/pii/092523129390006O},
author = {Shun-ichi Amari},
keywords = {Stochastic descent, generalized delta rule, dynamics of learning, pattern classification, multilayer perceptron},
abstract = {The backpropagation learning method has opened a way to wide applications of neural network research. It is a type of the stochastic descent method known in the sixties. The present paper reviews the wide applicability of the stochastic gradient descent method to various types of models and loss functions. In particular, we apply it to the pattern recognition problem, obtaining a new learning algorithm based on the information criterion. Dynamical properties of learning curves are then studied based on an old paper by the author where the stochastic descent method was proposed for general multilayer networks. The paper is concluded with a short section offering some historical remarks.}
}

@inproceedings{optuna_2019,
    title={Optuna: A Next-generation Hyperparameter Optimization Framework},
    author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
    booktitle={Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
    year={2019}
}


@misc{ronneberger2015unetconvolutionalnetworksbiomedical,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1505.04597}, 
}
@article{Schindelin2012,
  added-at = {2016-01-18T15:13:04.000+0100},
  author = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and others},
  biburl = {https://www.bibsonomy.org/bibtex/2f6cc22018bad4107eac157c1ac84ce99/fezett},
  file = {:/home/felix/Copy/Diplom/Docear/projects/Diplom/literature_repository/Fiji an open-source platform for biological-image analysis.pdf:PDF},
  interhash = {18badb95518a146d3d95815b7fdbb6f0},
  intrahash = {f6cc22018bad4107eac157c1ac84ce99},
  journal = {Nature methods},
  keywords = {imported},
  number = 7,
  pages = {676-682},
  publisher = {Nature Publishing Group},
  timestamp = {2016-01-18T15:15:28.000+0100},
  title = {Fiji: an open-source platform for biological-image analysis},
  volume = 9,
  year = 2012
}

@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}


@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}
@article{fritsch1980monotone,
  title={Monotone piecewise cubic interpolation},
  author={Fritsch, F. N. and Carlson, R. E.},
  journal={SIAM Journal on Numerical Analysis},
  volume={17},
  number={2},
  pages={238--246},
  year={1980},
  publisher={SIAM},
  doi={10.1137/0717021}
}
@article{lloyd1982least,
  author={Lloyd, Stuart},
  title={Least squares quantization in PCM},
  journal={IEEE Transactions on Information Theory},
  volume={28},
  number={2},
  pages={129-137},
  year={1982},
  publisher={IEEE},
  doi={10.1109/TIT.1982.1056489}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
@article{2018arXiv180906839B,
  author = {Buslaev, A. and Parinov, A. and Khvedchenya, E. and Iglovikov, V.~I. and Kalinin, A.~A.},  title = "{Albumentations: fast and flexible image augmentations}",
  journal = {ArXiv e-prints},
  eprint = {1809.06839},
  year = 2018
}

@misc{Iakubovskii:2019,
  Author = {Pavel Iakubovskii},
  Title = {Segmentation Models Pytorch},
  Year = {2019},
  Publisher = {GitHub},
  Journal = {GitHub repository},
  Howpublished = {\url{https://github.com/qubvel/segmentation_models.pytorch}}
}

@misc{Iakubovskii_2019,
  author = {Pavel Iakubovskii},
  title = {Segmentation Models PyTorch},
  year = {2019},
  url = {https://smp.readthedocs.io/en/latest/models.html#id21},
  note = {Accessed: 2025-02-04}
}
@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@Article{info11020125,
    AUTHOR = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
    TITLE = {Albumentations: Fast and Flexible Image Augmentations},
    JOURNAL = {Information},
    VOLUME = {11},
    YEAR = {2020},
    NUMBER = {2},
    ARTICLE-NUMBER = {125},
    URL = {https://www.mdpi.com/2078-2489/11/2/125},
    ISSN = {2078-2489},
    DOI = {10.3390/info11020125}
}

@misc{oshea2015introductionconvolutionalneuralnetworks,
      title={An Introduction to Convolutional Neural Networks}, 
      author={Keiron O'Shea and Ryan Nash},
      year={2015},
      eprint={1511.08458},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1511.08458}, 
}

@misc{plotly, author = {Plotly Technologies Inc.}, title = {Collaborative data science}, publisher = {Plotly Technologies Inc.}, address = {Montreal, QC}, year = {2015}, url = {https://plot.ly} }

@article{kirillov2023segany,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}

@misc{chatgpt2025,
  author = {OpenAI},
  title = {ChatGPT: Language Model for Text Generation and Assistance},
  year = {2025},
  url = {https://openai.com/chatgpt},
  note = {Accessed: 2025-02-19}
}
@misc{grammarly2025,
  author = {Grammarly Inc.},
  title = {Grammarly: AI-powered Writing Assistant},
  year = {2025},
  url = {https://www.grammarly.com},
  note = {Accessed: 2025-02-19}
}
