\chapter{Conclusion}

This thesis successfully addresses the challenge of automating a time-consuming manual process in the nuclear industry. Researchers are often required to perform repetitive, labor-intensive measurements. The primary goal was to develop a solution that integrates seamlessly into the researcher’s workflow without causing disruptions. The thesis focused on a specific category of coating-layer images with relatively homogeneous properties, in contrast to other images in the dataset. It was shown that segmentation of more complex, heterogeneous images is a viable direction for future work.

Three approaches were evaluated. First, the classical K-means algorithm was found to be computationally efficient, but it lacked universal applicability due to the variability of coating layers. This made it unsuitable for a streamlined workflow without extensive post-processing. Second, the Segment Anything Model (SAM) was considered, but hardware limitations made it infeasible for the given problem. Finally, Convolutional Neural Networks (CNNs) were explored. This solution required labeled data. Three types of labels were developed. The model was trained on the most precise set, but it was found that even the least manually intensive labels were effective for training, despite being less accurate. This insight suggests that efficient model retraining is also possible with this kind of labels and that the model could be adapted to handle more complex images in the future.

The optimization and testing of the model were thoroughly discussed, with results measured using the Intersection over Union (IoU) metric. The model achieved a mean IoU of 95.47\%, demonstrating its high accuracy in segmenting coating layers.

After integrating the model into the researcher’s workflow, the practical impact was clear. The time required for measurement tasks was reduced by half. For images similar to the training dataset, over 90\% of predicted measurements required no further manual adjustments. However, for images from batches with significantly different characteristics, performance declined, highlighting the need for further re-training.

With the code infrastructure in place, future improvements can focus on streamlining the training process and enhancing prediction accuracy. The current model was trained on fewer than 100 images from a limited number of batches, which is insufficient for creating a fully robust model. Nevertheless, even when incorrect, the model's average deviation was approximately five pixels, resulting in substantial time savings. This efficiency will accelerate the generation of additional labeled data, creating a feedback loop for continuous improvement. The iterative workflow is illustrated in figure~\ref{fig:workflow-diagram}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, every node/.style={align=center, font=\small}]

        \node[rectangle, draw, minimum width=3cm] (dataset) {Create datasets};
        \node[rectangle, draw, minimum width=3cm, below of=dataset, yshift=0cm] (train) {Train model};
        \node[rectangle, draw, minimum width=3cm, below of=train, yshift=-0.5cm] (integration) {Integrate to \ Fiji};
        \node[ellipse, draw, fill=blue!20, minimum width=3cm, below of=integration, yshift=-0.5cm] (result) {Workflow accelerated};

        % Arrows
        \draw[->, thick] (dataset) -- (train);
        \draw[->, thick] (train) -- (integration);
        \draw[->, thick] (integration) -- (result);
        \draw[->, thick, dotted] (result) to[bend right=80] (dataset);
    \end{tikzpicture}
    \caption{Workflow of model integration and performance improvement.}
    \label{fig:workflow-diagram}
\end{figure}

In conclusion, this thesis automates a portion of the researcher’s measurement process and establishes a pipeline for continuous model retraining and improvement. It also demonstrates that polygon masks, generated directly from researcher annotations, are sufficient for training segmentation models. These results suggest that similar models can be developed for other types of images within the researcher’s domain. The improvements brought by this work will lead to time savings and improved efficiency. This work highlights the practical impact of deep learning in automating image analysis tasks in the nuclear industry.

\chapter{Appendix}

The complete implementation of this thesis, including preprocessing, label generation, model training, and evaluation scripts, is available at [GitHub Repository Link (will be added in future when it has documentation, proper README,...)].

This project benefited from various open-source libraries that facilitated image processing, deep learning model training, and evaluation. A detailed list of dependencies and libraries used can be found in the repository’s requirements.txt file.

Additionally, this text was refined and grammar-checked using ChatGPT \cite{chatgpt2025}, and Grammarly \cite{grammarly2025}.

